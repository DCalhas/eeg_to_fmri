{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Cross Correlation between EEG and BOLD signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GOAL: perform convolution among bands and channels for each timestep, with non linear activations, in order to have a signal of the same length as the BOLD signal and correlate them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The loss will be the inverse cross correlation between the activation signal and BOLD signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import eeg_utils\n",
    "import fmri_utils\n",
    "import deep_cross_corr as deep_corr\n",
    "\n",
    "import numpy as np\n",
    "from numpy import correlate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mne\n",
    "from nilearn.masking import apply_mask, compute_epi_mask\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from scipy.signal import resample\n",
    "from nilearn.image import smooth_img, index_img, iter_img, clean_img, math_img, mean_img\n",
    "from nilearn import plotting\n",
    "from nilearn import image\n",
    "from nilearn.input_data import NiftiMasker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n"
     ]
    }
   ],
   "source": [
    "mask = fmri_utils.get_population_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/32/export/20130410320002_Segmentation_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 162022  =      0.000 ...   648.088 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/35/export/20130424350002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 197234  =      0.000 ...   788.936 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/36/export/20130425360002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 181949  =      0.000 ...   727.796 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/37/export/20130426370002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 195159  =      0.000 ...   780.636 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/38/export/20130105380002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 179384  =      0.000 ...   717.536 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/39/export/20130501390002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 182129  =      0.000 ...   728.516 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/40/export/20130510400002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 173914  =      0.000 ...   695.656 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/42/export/20130523420002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 184909  =      0.000 ...   739.636 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/43/export/20130529430002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 170594  =      0.000 ...   682.376 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/44/export/20130605440002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 169854  =      0.000 ...   679.416 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/45/export/20130627450002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 168099  =      0.000 ...   672.396 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(176, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/46/export/20130703460002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 172264  =      0.000 ...   689.056 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/47/export/20130710470002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 182594  =      0.000 ...   730.376 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/48/export/20130717480002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 171739  =      0.000 ...   686.956 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/49/export/20130918490002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 167579  =      0.000 ...   670.316 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/50/export/20131003_500002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 168019  =      0.000 ...   672.076 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG', 'O9', 'O10']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 14164, 20)\n",
      "(224, 64, 5, 20) (224, 14164, 20)\n"
     ]
    }
   ],
   "source": [
    "n_partitions = 16\n",
    "\n",
    "X_train, y_train = deep_corr.get_data(list(range(14)), masker=mask, n_partitions=n_partitions)\n",
    "\n",
    "X_test, y_test = deep_corr.get_data(list(range(14, 16)), masker=mask, n_partitions=n_partitions)\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lack of data, due to only having space for 100 instances:\n",
    "\n",
    "Our dataset has multiple X instances being equal with different target y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 5, 1)\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_31 (Conv3D)           (None, 63, 2, 20, 1)      5         \n",
      "_________________________________________________________________\n",
      "batch_normalization_54 (Batc (None, 63, 2, 20, 1)      4         \n",
      "_________________________________________________________________\n",
      "conv3d_32 (Conv3D)           (None, 62, 1, 20, 1)      5         \n",
      "_________________________________________________________________\n",
      "batch_normalization_55 (Batc (None, 62, 1, 20, 1)      4         \n",
      "_________________________________________________________________\n",
      "conv3d_33 (Conv3D)           (None, 61, 1, 20, 1)      3         \n",
      "_________________________________________________________________\n",
      "batch_normalization_56 (Batc (None, 61, 1, 20, 1)      4         \n",
      "_________________________________________________________________\n",
      "reshape_9 (Reshape)          (None, 61, 20, 1)         0         \n",
      "=================================================================\n",
      "Total params: 25\n",
      "Trainable params: 19\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n",
      "None\n",
      "(14164, 1)\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_25 (Conv2D)           (None, 282, 20, 1)        101       \n",
      "_________________________________________________________________\n",
      "batch_normalization_57 (Batc (None, 282, 20, 1)        4         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 61, 20, 1)         101       \n",
      "_________________________________________________________________\n",
      "batch_normalization_58 (Batc (None, 61, 20, 1)         4         \n",
      "=================================================================\n",
      "Total params: 210\n",
      "Trainable params: 206\n",
      "Non-trainable params: 4\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "from keras.initializers import Zeros\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, Conv3D, Reshape, Flatten, BatchNormalization, LSTM, TimeDistributed, Dense, Lambda, Input, MaxPooling2D, MaxPooling3D \n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import mae\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "#hyperparameters to be optimized:\n",
    "#n_partitions, output_dim, activation_function, reg_l, n_layers, dropout, maxpooling\n",
    "\n",
    "output_dim = 20\n",
    "activation_function = 'selu'\n",
    "reg_l=0\n",
    "learning_rate=0.001\n",
    "regularizer = regularizers.l1(reg_l)\n",
    "\n",
    "def eeg_network(input_shape, kernel_size, output_dim=20, activation_function='selu', regularizer=regularizers.l1(0.001)):\n",
    "    model = Sequential()\n",
    "\n",
    "\n",
    "    model.add(Conv3D(1, kernel_size=(2, 2, kernel_size[2]),\n",
    "        activation=activation_function, strides=(1,2,1),\n",
    "        input_shape=input_shape, kernel_regularizer=regularizer, \n",
    "        bias_regularizer=regularizer, activity_regularizer=regularizer))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv3D(1, kernel_size=(2, 2, kernel_size[2]),\n",
    "        activation=activation_function, strides=(1,2,1),\n",
    "        kernel_regularizer=regularizer, \n",
    "        bias_regularizer=regularizer, activity_regularizer=regularizer))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv3D(1, kernel_size=(2, 1, kernel_size[2]),\n",
    "        activation=activation_function, strides=(1,1,1),\n",
    "        kernel_regularizer=regularizer, \n",
    "        bias_regularizer=regularizer, activity_regularizer=regularizer))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Reshape((61, 20, 1)))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def bold_network(input_shape, kernel_size, output_dim=20, activation_function='selu', regularizer=regularizers.l1(0.001)):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(1, kernel_size=(100, kernel_size[1]),\n",
    "        activation=activation_function, strides=(50,1),\n",
    "        input_shape=input_shape, kernel_regularizer=regularizer, \n",
    "        bias_regularizer=regularizer, activity_regularizer=regularizer))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(1, kernel_size=(100, kernel_size[1]),\n",
    "        activation=activation_function, strides=(3,1),\n",
    "        kernel_regularizer=regularizer, \n",
    "        bias_regularizer=regularizer, activity_regularizer=regularizer))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    return model\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], X_train.shape[3], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], X_test.shape[3], 1)\n",
    "\n",
    "eeg_input_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3], 1)\n",
    "\n",
    "kernel_size = (X_train.shape[1], X_train.shape[2], 1)\n",
    "print(kernel_size)\n",
    "#eeg network\n",
    "eeg_network = eeg_network(eeg_input_shape, kernel_size, output_dim=output_dim, \n",
    "                          activation_function=activation_function, regularizer=regularizer)\n",
    "print(eeg_network.summary())\n",
    "\n",
    "#BOLD network\n",
    "y_train = y_train.reshape(y_train.shape[0], y_train.shape[1], y_train.shape[2], 1)\n",
    "y_test = y_test.reshape(y_test.shape[0], y_test.shape[1], y_test.shape[2], 1)\n",
    "\n",
    "bold_input_shape = (y_train.shape[1], y_train.shape[2], 1)\n",
    "\n",
    "kernel_size = (y_train.shape[1], 1)\n",
    "print(kernel_size)\n",
    "\n",
    "bold_network = bold_network(bold_input_shape, kernel_size, output_dim=output_dim, \n",
    "                            activation_function=activation_function, regularizer=regularizer)\n",
    "print(bold_network.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_eeg = Input(shape=eeg_input_shape)\n",
    "input_bold = Input(shape=bold_input_shape)\n",
    "\n",
    "# because we re-use the same instance `base_network`,\n",
    "# the weights of the network\n",
    "# will be shared across the two branches\n",
    "processed_eeg = eeg_network(input_eeg)\n",
    "processed_bold = bold_network(input_bold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's define a cross correlation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_correlation(x, y):\n",
    "    #how should the normalization be done??\n",
    "    x = K.l2_normalize(x, axis=1)\n",
    "    y = K.l2_normalize(y, axis=1)\n",
    "\n",
    "    a = K.batch_dot(x, y, axes=1)\n",
    "\n",
    "    b = K.batch_dot(x, x, axes=1)\n",
    "    c = K.batch_dot(y, y, axes=1)\n",
    "\n",
    "    return 1 - (a / (K.sqrt(b) * K.sqrt(c)))\n",
    "\n",
    "def correlation(vects):\n",
    "    #how should the normalization be done??\n",
    "    x, y = vects\n",
    "    x = K.l2_normalize(x, axis=1)\n",
    "    y = K.l2_normalize(y, axis=1)\n",
    "    \n",
    "    #flatten because we are dealing with 16x20 matrices\n",
    "    x = K.batch_flatten(x)\n",
    "    y = K.batch_flatten(y)\n",
    "\n",
    "    a = K.batch_dot(x, y, axes=1)\n",
    "\n",
    "    b = K.batch_dot(x, x, axes=1)\n",
    "    c = K.batch_dot(y, y, axes=1)\n",
    "\n",
    "    return 1 - (a / (K.sqrt(b) * K.sqrt(c)))\n",
    "\n",
    "def cos_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "        square_pred = K.square(y_pred)\n",
    "        margin_square = K.square(K.maximum(1.0 - y_pred, 0))\n",
    "        return K.mean(y_true * square_pred + (1 - y_true) * margin_square)\n",
    "\n",
    "correlation = Lambda(correlation,#compare this results with euclidean\n",
    "              output_shape=cos_dist_output_shape)([processed_eeg, \n",
    "              processed_bold])\n",
    "\n",
    "multi_modal_model = Model([input_eeg, input_bold], correlation)\n",
    "\n",
    "multi_modal_model.compile(loss=contrastive_loss, optimizer=Adam(lr=learning_rate))\n",
    "eeg_network.compile(loss=cross_correlation, optimizer=Adam(lr=0.0001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create positive and negative pairs of EEG and BOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eeg_bold_pairs(eeg, bold):\n",
    "    x_eeg = []\n",
    "    x_bold = []\n",
    "    y = []\n",
    "    \n",
    "    #how are we going to pair these? only different individuals??\n",
    "    #different timesteps of the same individual\n",
    "    \n",
    "    #redefine this variable\n",
    "    instances_per_individual = 8\n",
    "\n",
    "    #building pairs\n",
    "    for individual in range(int(len(eeg)/instances_per_individual)):\n",
    "        for other_individual in range(int(len(eeg)/instances_per_individual)):\n",
    "            for time_partitions in range(instances_per_individual):\n",
    "                if(individual == other_individual):\n",
    "                    x_eeg += [eeg[individual + time_partitions]]\n",
    "                    x_bold += [bold[other_individual + time_partitions]]\n",
    "                    y += [[1]]\n",
    "                else:\n",
    "                    x_eeg += [eeg[individual + time_partitions]]\n",
    "                    x_bold += [bold[other_individual + time_partitions]]\n",
    "                    y += [[0]]\n",
    "                    print\n",
    "    \n",
    "    x_eeg = np.array(x_eeg)\n",
    "    x_bold = np.array(x_bold)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return x_eeg, x_bold, y\n",
    "\n",
    "X_train_eeg, X_train_bold, tr_y = create_eeg_bold_pairs(X_train, y_train)\n",
    "X_test_eeg, X_test_bold, te_y = create_eeg_bold_pairs(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6272, 64, 5, 20, 1)\n",
      "(6272, 14164, 20, 1)\n",
      "(128, 64, 5, 20, 1)\n",
      "(128, 14164, 20, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_eeg.shape)\n",
    "print(X_train_bold.shape)\n",
    "\n",
    "print(X_test_eeg.shape)\n",
    "print(X_test_bold.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally train and see if it converges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6272 samples, validate on 128 samples\n",
      "Epoch 1/100\n",
      "6272/6272 [==============================] - 20s 3ms/step - loss: 0.0366 - val_loss: 0.2317\n",
      "Epoch 2/100\n",
      "6272/6272 [==============================] - 19s 3ms/step - loss: 0.0351 - val_loss: 0.2636\n",
      "Epoch 3/100\n",
      "6272/6272 [==============================] - 19s 3ms/step - loss: 0.0346 - val_loss: 0.2547\n",
      "Epoch 4/100\n",
      "6272/6272 [==============================] - 19s 3ms/step - loss: 0.0341 - val_loss: 0.2492\n",
      "Epoch 5/100\n",
      "6272/6272 [==============================] - 19s 3ms/step - loss: 0.0338 - val_loss: 0.2478\n",
      "Epoch 6/100\n",
      "6272/6272 [==============================] - 19s 3ms/step - loss: 0.0335 - val_loss: 0.2465\n",
      "Epoch 7/100\n",
      "6272/6272 [==============================] - 19s 3ms/step - loss: 0.0333 - val_loss: 0.2356\n",
      "Epoch 8/100\n",
      "6272/6272 [==============================] - 19s 3ms/step - loss: 0.0330 - val_loss: 0.2331\n",
      "Epoch 9/100\n",
      "6272/6272 [==============================] - 19s 3ms/step - loss: 0.0328 - val_loss: 0.2316\n",
      "Epoch 10/100\n",
      "6272/6272 [==============================] - 19s 3ms/step - loss: 0.0326 - val_loss: 0.2207\n",
      "Epoch 11/100\n",
      "6272/6272 [==============================] - 19s 3ms/step - loss: 0.0324 - val_loss: 0.2147\n",
      "Epoch 12/100\n",
      "6272/6272 [==============================] - 19s 3ms/step - loss: 0.0321 - val_loss: 0.2121\n",
      "Epoch 13/100\n",
      "6272/6272 [==============================] - 19s 3ms/step - loss: 0.0320 - val_loss: 0.2112\n",
      "Epoch 14/100\n",
      "6272/6272 [==============================] - 19s 3ms/step - loss: 0.0319 - val_loss: 0.2179\n",
      "Epoch 15/100\n",
      "6272/6272 [==============================] - 19s 3ms/step - loss: 0.0317 - val_loss: 0.2231\n",
      "Epoch 16/100\n",
      "6272/6272 [==============================] - 19s 3ms/step - loss: 0.0315 - val_loss: 0.2315\n",
      "Epoch 17/100\n",
      "6272/6272 [==============================] - 19s 3ms/step - loss: 0.0313 - val_loss: 0.2370\n",
      "Epoch 18/100\n",
      "6272/6272 [==============================] - 19s 3ms/step - loss: 0.0311 - val_loss: 0.2436\n",
      "Epoch 19/100\n",
      "6272/6272 [==============================] - 19s 3ms/step - loss: 0.0311 - val_loss: 0.2523\n",
      "Epoch 20/100\n",
      "6272/6272 [==============================] - 19s 3ms/step - loss: 0.0309 - val_loss: 0.2577\n",
      "Epoch 21/100\n",
      "5120/6272 [=======================>......] - ETA: 3s - loss: 0.0306"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-adbf6061e55f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m history = multi_modal_model.fit([X_train_eeg, X_train_bold], tr_y, \n\u001b[1;32m      2\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                     batch_size=n_partitions*16, validation_data=([X_test_eeg, X_test_bold], te_y))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3625\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3626\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3627\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3629\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = multi_modal_model.fit([X_train_eeg, X_train_bold], tr_y, \n",
    "                    epochs=15, \n",
    "                    batch_size=n_partitions*16, validation_data=([X_test_eeg, X_test_bold], te_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = eeg_network.to_json()\n",
    "with open(\"../multi_model/eeg_demo_0.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "eeg_network.save_weights(\"../multi_model/eeg_demo_0.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], color='b')\n",
    "plt.plot(history.history['val_loss'], color='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the activations and plot the cross-correlation coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_embeddings = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_corr = correlate(eeg_embeddings[0], y[0])\n",
    "linear_corr = correlate(X[0][0][0].reshape(1,-1)[0], y[0])\n",
    "print(deep_corr, linear_corr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
