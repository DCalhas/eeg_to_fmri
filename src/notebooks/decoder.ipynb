{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This model decodes EEG embeddings from another network to fMRI signal. This other network learns a space shared between EEG and fMRI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import eeg_utils\n",
    "import fmri_utils\n",
    "import deep_cross_corr\n",
    "\n",
    "import numpy as np\n",
    "from numpy import correlate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mne\n",
    "from nilearn.masking import apply_mask, compute_epi_mask\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from scipy.signal import resample\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.initializers import Zeros\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, Conv3D, Conv2DTranspose, ZeroPadding2D, Reshape, Flatten, BatchNormalization, LSTM, TimeDistributed, Dense, Lambda, Input, MaxPooling2D, MaxPooling3D \n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import mae\n",
    "from keras.models import model_from_json\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/32/export/20130410320002_Segmentation_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 162022  =      0.000 ...   648.088 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/35/export/20130424350002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 197234  =      0.000 ...   788.936 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/36/export/20130425360002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 181949  =      0.000 ...   727.796 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/37/export/20130426370002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 195159  =      0.000 ...   780.636 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/38/export/20130105380002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 179384  =      0.000 ...   717.536 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/39/export/20130501390002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 182129  =      0.000 ...   728.516 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/40/export/20130510400002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 173914  =      0.000 ...   695.656 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/42/export/20130523420002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 184909  =      0.000 ...   739.636 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/43/export/20130529430002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 170594  =      0.000 ...   682.376 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/44/export/20130605440002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 169854  =      0.000 ...   679.416 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/45/export/20130627450002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 168099  =      0.000 ...   672.396 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(176, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/46/export/20130703460002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 172264  =      0.000 ...   689.056 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/47/export/20130710470002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 182594  =      0.000 ...   730.376 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/48/export/20130717480002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 171739  =      0.000 ...   686.956 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/49/export/20130918490002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 167579  =      0.000 ...   670.316 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/50/export/20131003_500002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 168019  =      0.000 ...   672.076 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG', 'O9', 'O10']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 14164, 20)\n"
     ]
    }
   ],
   "source": [
    "mask = fmri_utils.get_population_mask()\n",
    "\n",
    "#reading data and spliting data into train and test by individuals\n",
    "eeg_train, bold_train = deep_cross_corr.get_data(list(range(14)), masker=mask)\n",
    "eeg_test, bold_test = deep_cross_corr.get_data(list(range(14, 16)), masker=mask)\n",
    "\n",
    "eeg_train = eeg_train.reshape(eeg_train.shape[0], eeg_train.shape[1], eeg_train.shape[2], eeg_train.shape[3], 1)\n",
    "eeg_test = eeg_test.reshape(eeg_test.shape[0], eeg_test.shape[1], eeg_test.shape[2], eeg_test.shape[3], 1)\n",
    "\n",
    "bold_train = bold_train.reshape(bold_train.shape[0], bold_train.shape[1], bold_train.shape[2], 1)\n",
    "bold_test = bold_test.reshape(bold_test.shape[0], bold_test.shape[1], bold_test.shape[2], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load EEG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_31 (Conv3D)           (None, 63, 2, 20, 1)      5         \n",
      "_________________________________________________________________\n",
      "batch_normalization_54 (Batc (None, 63, 2, 20, 1)      4         \n",
      "_________________________________________________________________\n",
      "conv3d_32 (Conv3D)           (None, 62, 1, 20, 1)      5         \n",
      "_________________________________________________________________\n",
      "batch_normalization_55 (Batc (None, 62, 1, 20, 1)      4         \n",
      "_________________________________________________________________\n",
      "conv3d_33 (Conv3D)           (None, 61, 1, 20, 1)      3         \n",
      "_________________________________________________________________\n",
      "batch_normalization_56 (Batc (None, 61, 1, 20, 1)      4         \n",
      "_________________________________________________________________\n",
      "reshape_9 (Reshape)          (None, 61, 20, 1)         0         \n",
      "=================================================================\n",
      "Total params: 25\n",
      "Trainable params: 19\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#LOAD MODEL WEIGHTS - EEG NETWORK\n",
    "saved_models_path = '../multi_model/'\n",
    "json_file = open(saved_models_path + 'eeg_demo_0.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "eeg_network = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "eeg_network.load_weights(saved_models_path + \"eeg_demo_0.h5\")\n",
    "\n",
    "print(eeg_network.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load activations from compression network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_eeg_train = eeg_network.predict(eeg_train)\n",
    "shared_eeg_test = eeg_network.predict(eeg_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the decoding neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_transpose_1 (Conv2DTr (None, 280, 20, 1)        101       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 280, 20, 1)        4         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 14050, 20, 1)      101       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14050, 20, 1)      4         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 14164, 20, 1)      0         \n",
      "=================================================================\n",
      "Total params: 210\n",
      "Trainable params: 206\n",
      "Non-trainable params: 4\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_shape = (None, shared_eeg_train.shape[1], shared_eeg_train.shape[2], 1)\n",
    "\n",
    "decoder_model = Sequential()\n",
    "\n",
    "decoder_model.add(Conv2DTranspose(1, kernel_size=(100, 1),\n",
    "                          activation='selu', strides=(3,1)))\n",
    "decoder_model.add(BatchNormalization())\n",
    "decoder_model.add(Conv2DTranspose(1, kernel_size=(100, 1), \n",
    "                          activation='selu', strides=(50,1)))\n",
    "decoder_model.add(BatchNormalization())\n",
    "#can we arrange another solution for the padding??\n",
    "decoder_model.add(ZeroPadding2D(padding=(57,0)))\n",
    "\n",
    "decoder_model.build(input_shape)\n",
    "\n",
    "print(decoder_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder_model.compile(optimizer='adam', loss=deep_cross_corr.cross_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(shared_eeg_train, bold_train, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So the encoder and decoder model are being trained separately, let's see if we train them at the same time, and propagate the loss of the decoder to the enconder, the performance increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder and Decoder training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 63, 2, 20, 1)      5         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 63, 2, 20, 1)      4         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 62, 1, 20, 1)      5         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 62, 1, 20, 1)      4         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 61, 1, 20, 1)      3         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 61, 1, 20, 1)      4         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 61, 20, 1)         0         \n",
      "=================================================================\n",
      "Total params: 25\n",
      "Trainable params: 19\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 282, 20, 1)        101       \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 282, 20, 1)        4         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 61, 20, 1)         101       \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 61, 20, 1)         4         \n",
      "=================================================================\n",
      "Total params: 210\n",
      "Trainable params: 206\n",
      "Non-trainable params: 4\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "eeg_input_shape = (eeg_train.shape[1], eeg_train.shape[2], eeg_train.shape[3], 1)\n",
    "kernel_size = (eeg_train.shape[1], eeg_train.shape[2], 1)\n",
    "eeg_network = deep_cross_corr.eeg_network(eeg_input_shape, kernel_size)\n",
    "print(eeg_network.summary())\n",
    "\n",
    "\n",
    "\n",
    "bold_input_shape = (bold_train.shape[1], bold_train.shape[2], 1)\n",
    "kernel_size = (bold_train.shape[1], 1)\n",
    "bold_network = deep_cross_corr.bold_network(bold_input_shape, kernel_size)\n",
    "print(bold_network.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_eeg = Input(shape=eeg_input_shape)\n",
    "input_bold = Input(shape=bold_input_shape)\n",
    "\n",
    "# because we re-use the same instance `base_network`,\n",
    "# the weights of the network\n",
    "# will be shared across the two branches\n",
    "processed_eeg = eeg_network(input_eeg)\n",
    "processed_bold = bold_network(input_bold)\n",
    "\n",
    "correlation = Lambda(deep_cross_corr.correlation, \n",
    "                     output_shape=deep_cross_corr.cos_dist_output_shape, name=\"correlation_layer\")([processed_eeg, processed_bold])\n",
    "\n",
    "decoded_bold = decoder_model(processed_eeg)\n",
    "\n",
    "\n",
    "\n",
    "#multi_modal_model = Model([input_eeg, input_bold], correlation)\n",
    "\n",
    "#alpha = 0.5\n",
    "#corr_loss = deep_cross_corr.contrastive_loss\n",
    "#decod_loss = deep_cross_corr.correlation\n",
    "#multi_modal_model.compile(loss={\"correlation_layer\":corr_loss, \"decoding_layer\": decod_loss], loss_weights=[alpha, 1-alpha], optimizer=Adam(lr=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_decoder = Model(inputs=[input_eeg, input_bold], outputs=[correlation, decoded_bold])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "corr_loss = deep_cross_corr.contrastive_loss\n",
    "decod_loss = deep_cross_corr.correlation_decoder_loss\n",
    "correlation_decoder.compile(loss=[corr_loss, decod_loss], loss_weights=[1, 1], optimizer=Adam(lr=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_eeg, X_train_bold, tr_y = deep_cross_corr.create_eeg_bold_pairs(eeg_train, bold_train)\n",
    "X_test_eeg, X_test_bold, te_y = deep_cross_corr.create_eeg_bold_pairs(eeg_test, bold_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/20\n",
      "3136/3136 [==============================] - 42s 13ms/step - loss: 195.4607 - correlation_layer_loss: 0.0764 - sequential_1_loss: 0.9998\n",
      "Epoch 2/20\n",
      "3136/3136 [==============================] - 40s 13ms/step - loss: 181.5771 - correlation_layer_loss: 0.0759 - sequential_1_loss: 0.9973\n",
      "Epoch 3/20\n",
      "3136/3136 [==============================] - 40s 13ms/step - loss: 168.0427 - correlation_layer_loss: 0.0753 - sequential_1_loss: 0.9948\n",
      "Epoch 4/20\n",
      "3136/3136 [==============================] - 40s 13ms/step - loss: 154.4216 - correlation_layer_loss: 0.0749 - sequential_1_loss: 0.9922\n",
      "Epoch 5/20\n",
      "3136/3136 [==============================] - 40s 13ms/step - loss: 140.6175 - correlation_layer_loss: 0.0746 - sequential_1_loss: 0.9896\n",
      "Epoch 6/20\n",
      "3136/3136 [==============================] - 40s 13ms/step - loss: 126.5445 - correlation_layer_loss: 0.0742 - sequential_1_loss: 0.9870\n",
      "Epoch 7/20\n",
      "3136/3136 [==============================] - 40s 13ms/step - loss: 112.2276 - correlation_layer_loss: 0.0739 - sequential_1_loss: 0.9846\n",
      "Epoch 8/20\n",
      "3136/3136 [==============================] - 40s 13ms/step - loss: 97.8016 - correlation_layer_loss: 0.0734 - sequential_1_loss: 0.9823\n",
      "Epoch 9/20\n",
      "3136/3136 [==============================] - 40s 13ms/step - loss: 83.4192 - correlation_layer_loss: 0.0731 - sequential_1_loss: 0.9801\n",
      "Epoch 10/20\n",
      "3136/3136 [==============================] - 40s 13ms/step - loss: 69.2794 - correlation_layer_loss: 0.0727 - sequential_1_loss: 0.9781\n",
      "Epoch 11/20\n",
      "3136/3136 [==============================] - 40s 13ms/step - loss: 55.5257 - correlation_layer_loss: 0.0719 - sequential_1_loss: 0.9763\n",
      "Epoch 12/20\n",
      "3136/3136 [==============================] - 40s 13ms/step - loss: 42.2978 - correlation_layer_loss: 0.0708 - sequential_1_loss: 0.9746\n",
      "Epoch 13/20\n",
      "3136/3136 [==============================] - 40s 13ms/step - loss: 29.7525 - correlation_layer_loss: 0.0698 - sequential_1_loss: 0.9731\n",
      "Epoch 14/20\n",
      "3136/3136 [==============================] - 40s 13ms/step - loss: 18.8304 - correlation_layer_loss: 0.0713 - sequential_1_loss: 0.9717\n",
      "Epoch 15/20\n",
      "3136/3136 [==============================] - 40s 13ms/step - loss: 16.4187 - correlation_layer_loss: 0.0729 - sequential_1_loss: 0.9705\n",
      "Epoch 16/20\n",
      "3136/3136 [==============================] - 40s 13ms/step - loss: 16.1028 - correlation_layer_loss: 0.0702 - sequential_1_loss: 0.9693\n",
      "Epoch 17/20\n",
      "3136/3136 [==============================] - 40s 13ms/step - loss: 15.7754 - correlation_layer_loss: 0.0694 - sequential_1_loss: 0.9683\n",
      "Epoch 18/20\n",
      "3136/3136 [==============================] - 40s 13ms/step - loss: 15.4578 - correlation_layer_loss: 0.0692 - sequential_1_loss: 0.9673\n",
      "Epoch 19/20\n",
      "3136/3136 [==============================] - 40s 13ms/step - loss: 15.1475 - correlation_layer_loss: 0.0688 - sequential_1_loss: 0.9664\n",
      "Epoch 20/20\n",
      "3136/3136 [==============================] - 39s 13ms/step - loss: 14.8564 - correlation_layer_loss: 0.0687 - sequential_1_loss: 0.9655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fbfc460ee10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_decoder.fit([X_train_eeg, X_train_bold], [tr_y, X_train_bold], \n",
    "                       epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
