{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This model decodes EEG embeddings from another network to fMRI signal. This other network learns a space shared between EEG and fMRI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import eeg_utils\n",
    "import fmri_utils\n",
    "import deep_cross_corr\n",
    "\n",
    "import numpy as np\n",
    "from numpy import correlate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mne\n",
    "from nilearn.masking import apply_mask, compute_epi_mask\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from scipy.signal import resample\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/32/export/20130410320002_Segmentation_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 162022  =      0.000 ...   648.088 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/35/export/20130424350002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 197234  =      0.000 ...   788.936 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/36/export/20130425360002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 181949  =      0.000 ...   727.796 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/37/export/20130426370002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 195159  =      0.000 ...   780.636 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/38/export/20130105380002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 179384  =      0.000 ...   717.536 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/39/export/20130501390002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 182129  =      0.000 ...   728.516 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/40/export/20130510400002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 173914  =      0.000 ...   695.656 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/42/export/20130523420002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 184909  =      0.000 ...   739.636 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/43/export/20130529430002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 170594  =      0.000 ...   682.376 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/44/export/20130605440002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 169854  =      0.000 ...   679.416 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/45/export/20130627450002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 168099  =      0.000 ...   672.396 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(176, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/46/export/20130703460002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 172264  =      0.000 ...   689.056 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/47/export/20130710470002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 182594  =      0.000 ...   730.376 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/48/export/20130717480002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 171739  =      0.000 ...   686.956 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/49/export/20130918490002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 167579  =      0.000 ...   670.316 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 14164, 20)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/50/export/20131003_500002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 168019  =      0.000 ...   672.076 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../eeg_utils.py:35: RuntimeWarning: No coordinate information found for channels ['ECG', 'O9', 'O10']. Setting channel types to misc. To avoid this warning, set channel types explicitly.\n",
      "  return mne.io.read_raw_brainvision(complete_path, preload=True)\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/nilearn/image/resampling.py:510: UserWarning: Casting data from int16 to float32\n",
      "  warnings.warn(\"Casting data from %s to %s\" % (data.dtype.name, aux))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 14164, 20)\n"
     ]
    }
   ],
   "source": [
    "mask = fmri_utils.get_population_mask()\n",
    "\n",
    "#reading data and spliting data into train and test by individuals\n",
    "eeg_train, bold_train = deep_cross_corr.get_data(list(range(14)), masker=mask)\n",
    "eeg_test, bold_test = deep_cross_corr.get_data(list(range(14, 16)), masker=mask)\n",
    "\n",
    "eeg_train = eeg_train.reshape(eeg_train.shape[0], eeg_train.shape[1], eeg_train.shape[2], eeg_train.shape[3], 1)\n",
    "eeg_test = eeg_test.reshape(eeg_test.shape[0], eeg_test.shape[1], eeg_test.shape[2], eeg_test.shape[3], 1)\n",
    "\n",
    "bold_train = bold_train.reshape(bold_train.shape[0], bold_train.shape[1], bold_train.shape[2], 1)\n",
    "bold_test = bold_test.reshape(bold_test.shape[0], bold_test.shape[1], bold_test.shape[2], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So the encoder and decoder model are being trained separately, let's see if we train them at the same time, and propagate the loss of the decoder to the enconder, the performance increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder and Decoder training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:4075: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d (Conv3D)              (None, 63, 2, 20, 1)      5         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 63, 2, 20, 1)      4         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 62, 1, 20, 1)      5         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 62, 1, 20, 1)      4         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 61, 1, 20, 1)      3         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 61, 1, 20, 1)      4         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 61, 20, 1)         0         \n",
      "=================================================================\n",
      "Total params: 25\n",
      "Trainable params: 19\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 282, 20, 1)        101       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 282, 20, 1)        4         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 61, 20, 1)         101       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 61, 20, 1)         4         \n",
      "=================================================================\n",
      "Total params: 210\n",
      "Trainable params: 206\n",
      "Non-trainable params: 4\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "eeg_input_shape = (eeg_train.shape[1], eeg_train.shape[2], eeg_train.shape[3], 1)\n",
    "kernel_size = (eeg_train.shape[1], eeg_train.shape[2], 1)\n",
    "eeg_network = deep_cross_corr.eeg_network(eeg_input_shape, kernel_size)\n",
    "print(eeg_network.summary())\n",
    "\n",
    "\n",
    "\n",
    "bold_input_shape = (bold_train.shape[1], bold_train.shape[2], 1)\n",
    "kernel_size = (bold_train.shape[1], 1)\n",
    "bold_network = deep_cross_corr.bold_network(bold_input_shape, kernel_size)\n",
    "print(bold_network.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_eeg = tf.keras.layers.Input(shape=eeg_input_shape)\n",
    "input_bold = tf.keras.layers.Input(shape=bold_input_shape)\n",
    "\n",
    "# because we re-use the same instance `base_network`,\n",
    "# the weights of the network\n",
    "# will be shared across the two branches\n",
    "processed_eeg = eeg_network(input_eeg)\n",
    "processed_bold = bold_network(input_bold)\n",
    "\n",
    "correlation = tf.keras.layers.Lambda(deep_cross_corr.correlation, \n",
    "                     output_shape=deep_cross_corr.cos_dist_output_shape, name=\"correlation_layer\")([processed_eeg, processed_bold])\n",
    "\n",
    "multi_modal_model = tf.keras.Model([input_eeg, input_bold], correlation)\n",
    "\n",
    "#alpha = 0.5\n",
    "#corr_loss = deep_cross_corr.contrastive_loss\n",
    "#decod_loss = deep_cross_corr.correlation\n",
    "#multi_modal_model.compile(loss={\"correlation_layer\":corr_loss, \"decoding_layer\": decod_loss], loss_weights=[alpha, 1-alpha], optimizer=Adam(lr=0.0001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load activations from compression network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_eeg_train = eeg_network.predict(eeg_train)\n",
    "shared_eeg_test = eeg_network.predict(eeg_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the decoding neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bold_synthesizer\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_transpose (Conv2DTran (None, 280, 20, 1)        101       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 280, 20, 1)        4         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 14050, 20, 1)      101       \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 14050, 20, 1)      4         \n",
      "_________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2 (None, 14164, 20, 1)      0         \n",
      "=================================================================\n",
      "Total params: 210\n",
      "Trainable params: 206\n",
      "Non-trainable params: 4\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_shape = (None, shared_eeg_train.shape[1], shared_eeg_train.shape[2], 1)\n",
    "\n",
    "decoder_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2DTranspose(1, kernel_size=(100, 1),\n",
    "                          activation='selu', strides=(3,1), input_shape=input_shape[1:]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2DTranspose(1, kernel_size=(100, 1), \n",
    "                          activation='selu', strides=(50,1)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ZeroPadding2D(padding=(57,0))\n",
    "], name=\"bold_synthesizer\")\n",
    "\n",
    "decoder_model.build(input_shape)\n",
    "\n",
    "print(decoder_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder_model.compile(optimizer='adam', loss=deep_cross_corr.cross_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(shared_eeg_train, bold_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation_decoder = Model(inputs=[input_eeg, input_bold], outputs=[correlation, last_layer_decoder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha = 0.5\n",
    "#corr_loss = deep_cross_corr.contrastive_loss\n",
    "#decod_loss = deep_cross_corr.correlation_decoder_loss\n",
    "#correlation_decoder.compile(loss=[corr_loss, decod_loss], loss_weights=[1, 1], optimizer=Adam(lr=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_eeg, X_train_bold, tr_y = deep_cross_corr.create_eeg_bold_pairs(eeg_train, bold_train)\n",
    "X_test_eeg, X_test_bold, te_y = deep_cross_corr.create_eeg_bold_pairs(eeg_test, bold_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation_decoder.fit([X_train_eeg, X_train_bold], [tr_y, X_train_bold], \n",
    "#                       epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model is not training as supposed, so a custom training is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the customized training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_eeg = tf.convert_to_tensor(X_train_eeg, dtype=np.float32)\n",
    "X_train_bold = tf.convert_to_tensor(X_train_bold, dtype=np.float32)\n",
    "tr_y = tf.convert_to_tensor(tr_y, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_decoder(model, inputs, targets):\n",
    "    reconstruction_loss = deep_cross_corr.cross_correlation(outputs, targets)\n",
    "    return K.mean(reconstruction_loss)\n",
    "\n",
    "\n",
    "def grad_decoder(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape:    \n",
    "        tape.watch(inputs)\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        #loss\n",
    "        reconstruction_loss = deep_cross_corr.cross_correlation(outputs, targets)\n",
    "        reconstruction_loss = K.mean(reconstruction_loss)\n",
    "        return -reconstruction_loss,  tape.gradient(-reconstruction_loss, model.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_multi_encoder(model, inputs, targets, reconstruction_loss):\n",
    "    with tf.GradientTape() as tape:    \n",
    "        tape.watch(inputs)\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        #loss\n",
    "        encoder_loss = abs(deep_cross_corr.contrastive_loss(outputs, targets)) + abs(reconstruction_loss)\n",
    "        return -encoder_loss,  tape.gradient(-encoder_loss, \n",
    "                                            model.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_training_loss:\n",
    "    def __init__(self):\n",
    "        self.encoder_loss = 0\n",
    "        self.decoder_loss = 0\n",
    "        self.batch = 0\n",
    "        \n",
    "    def update_batch_decoder_loss_avg(self, loss):\n",
    "        self.decoder_loss += loss\n",
    "        self.batch += 1\n",
    "    \n",
    "    def get_batch_decoder_loss_avg(self):\n",
    "        return self.decoder_loss/self.batch\n",
    "    \n",
    "    def update_batch_encoder_loss_avg(self, loss):\n",
    "        self.encoder_loss += loss\n",
    "        self.batch += 1\n",
    "    \n",
    "    def get_batch_encoder_loss_avg(self):\n",
    "        return self.encoder_loss/self.batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f8b09ece16c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Optimize the synthesizer model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mdecoder_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared_eeg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_bold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_stop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         optimizer.apply_gradients(zip(decoder_grads, decoder_model.trainable_variables), \n\u001b[1;32m     28\u001b[0m                                   global_step)\n",
      "\u001b[0;32m<ipython-input-14-ecadea080769>\u001b[0m in \u001b[0;36mgrad_decoder\u001b[0;34m(model, inputs, targets)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mreconstruction_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeep_cross_corr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_correlation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mreconstruction_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstruction_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mreconstruction_loss\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mreconstruction_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m def _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs,\n\u001b[0m\u001b[1;32m    116\u001b[0m                        out_grads, skip_input_indices):\n\u001b[1;32m    117\u001b[0m   \"\"\"Calls the gradient function of the op.\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# keep results for plotting\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "global_step = tf.Variable(0)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    losses = custom_training_loss()\n",
    "    # Training loop - using batches of 32\n",
    "    batch_size = 128\n",
    "    for batch_init in range(0, len(X_train_eeg), batch_size):\n",
    "        batch_start = batch_init\n",
    "        if(batch_start + batch_size >= len(X_train_eeg)):\n",
    "            batch_stop = len(X_train_eeg)\n",
    "        else:\n",
    "            batch_stop = batch_start + batch_size\n",
    "        \n",
    "        shared_eeg = eeg_network(X_train_eeg[batch_start:batch_stop])\n",
    "        \n",
    "        # Optimize the synthesizer model\n",
    "        decoder_loss, decoder_grads = grad_decoder(decoder_model, shared_eeg, X_train_bold[batch_start:batch_stop])\n",
    "        optimizer.apply_gradients(zip(decoder_grads, decoder_model.trainable_variables), \n",
    "                                  global_step)\n",
    "\n",
    "        \n",
    "        #now train the compression by correlation model\n",
    "        encoder_loss, encoder_grads = grad_multi_encoder(multi_modal_model, \n",
    "                                                         [X_train_eeg[batch_start:batch_stop], \n",
    "                                                                             X_train_bold[batch_start:batch_stop]], \n",
    "                                                         tr_y[batch_start:batch_stop], decoder_loss)\n",
    "        optimizer.apply_gradients(zip(encoder_grads, multi_modal_model.trainable_variables), \n",
    "                                  global_step)\n",
    "        # Track progress\n",
    "        losses.update_batch_decoder_loss_avg(decoder_loss)\n",
    "        losses.update_batch_encoder_loss_avg(encoder_loss)\n",
    "\n",
    "    # end epoch\n",
    "    decoder_loss = losses.get_batch_decoder_loss_avg()\n",
    "    encoder_loss = losses.get_batch_encoder_loss_avg()\n",
    "    \n",
    "    print(encoder_loss, decoder_loss)\n",
    "    #if epoch % 50 == 0:\n",
    "    #    print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch, \n",
    "    #                                                                epoch_loss_avg.result(), \n",
    "    #                                                                epoch_accuracy.result()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
