{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/davidcalhas/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/32/export/20130410320002_Segmentation_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 162022  =      0.000 ...   648.088 secs...\n",
      "(50, 2607, 7)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/35/export/20130424350002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 197234  =      0.000 ...   788.936 secs...\n",
      "(100, 2607, 7)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/36/export/20130425360002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 181949  =      0.000 ...   727.796 secs...\n",
      "(150, 2607, 7)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/37/export/20130426370002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 195159  =      0.000 ...   780.636 secs...\n",
      "(200, 2607, 7)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/38/export/20130105380002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 179384  =      0.000 ...   717.536 secs...\n",
      "(250, 2607, 7)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/39/export/20130501390002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 182129  =      0.000 ...   728.516 secs...\n",
      "(300, 2607, 7)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/40/export/20130510400002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 173914  =      0.000 ...   695.656 secs...\n",
      "(350, 2607, 7)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/42/export/20130523420002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 184909  =      0.000 ...   739.636 secs...\n",
      "(400, 2607, 7)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/43/export/20130529430002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 170594  =      0.000 ...   682.376 secs...\n",
      "(450, 2607, 7)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/44/export/20130605440002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 169854  =      0.000 ...   679.416 secs...\n",
      "(500, 2607, 7)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/45/export/20130627450002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 168099  =      0.000 ...   672.396 secs...\n",
      "(50, 2607, 7)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/46/export/20130703460002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 172264  =      0.000 ...   689.056 secs...\n",
      "(100, 2607, 7)\n",
      "Finished Loading Data\n",
      "Pairs Created\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/32/export/20130410320002_Segmentation_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 162022  =      0.000 ...   648.088 secs...\n",
      "(25, 2607, 14)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/35/export/20130424350002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 197234  =      0.000 ...   788.936 secs...\n",
      "(50, 2607, 14)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/36/export/20130425360002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 181949  =      0.000 ...   727.796 secs...\n",
      "(75, 2607, 14)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/37/export/20130426370002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 195159  =      0.000 ...   780.636 secs...\n",
      "(100, 2607, 14)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/38/export/20130105380002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 179384  =      0.000 ...   717.536 secs...\n",
      "(125, 2607, 14)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/39/export/20130501390002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 182129  =      0.000 ...   728.516 secs...\n",
      "(150, 2607, 14)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/40/export/20130510400002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 173914  =      0.000 ...   695.656 secs...\n",
      "(175, 2607, 14)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/42/export/20130523420002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 184909  =      0.000 ...   739.636 secs...\n",
      "(200, 2607, 14)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/43/export/20130529430002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 170594  =      0.000 ...   682.376 secs...\n",
      "(225, 2607, 14)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/44/export/20130605440002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 169854  =      0.000 ...   679.416 secs...\n",
      "(250, 2607, 14)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/45/export/20130627450002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 168099  =      0.000 ...   672.396 secs...\n",
      "(25, 2607, 14)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/46/export/20130703460002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 172264  =      0.000 ...   689.056 secs...\n",
      "(50, 2607, 14)\n",
      "Finished Loading Data\n",
      "Pairs Created\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/47/export/20130710470002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 182594  =      0.000 ...   730.376 secs...\n",
      "(25, 2607, 14)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/48/export/20130717480002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 171739  =      0.000 ...   686.956 secs...\n",
      "(50, 2607, 14)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/49/export/20130918490002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 167579  =      0.000 ...   670.316 secs...\n",
      "(75, 2607, 14)\n",
      "Extracting parameters from /home/davidcalhas/eeg_to_fmri/datasets/01/EEG/50/export/20131003_500002_Pulse_Artifact_Correction_bin.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 168019  =      0.000 ...   672.076 secs...\n",
      "(100, 2607, 14)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../..')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import utils.losses_utils as losses\n",
    "\n",
    "import utils.data_utils as data_utils\n",
    "\n",
    "import utils.viz_utils as viz\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "bold_shift=3\n",
    "f_resample=1.8\n",
    "n_partitions=25\n",
    "activation=''\n",
    "\n",
    "eeg_train, bold_train, eeg_val, bold_val = data_utils.load_data(list(range(10)), \n",
    "                                                                list(range(10, 12)), \n",
    "                                                                bold_shift=bold_shift, \n",
    "                                                                n_partitions=n_partitions, f_resample=f_resample,\n",
    "                                                                roi=1, roi_ica_components=20)\n",
    "\n",
    "#standardize data\n",
    "eeg_train, bold_train, eeg_scaler, bold_scaler = data_utils.standardize(eeg_train, bold_train)\n",
    "eeg_val, bold_val, _, _ = data_utils.standardize(eeg_val, bold_val, eeg_scaler=eeg_scaler, bold_scaler=bold_scaler)\n",
    "\n",
    "n_voxels = bold_train.shape[1]\n",
    "\n",
    "print(\"Finished Loading Data\")\n",
    "\n",
    "X_train_eeg, X_train_bold, tr_y = data_utils.create_eeg_bold_pairs(eeg_train, bold_train)\n",
    "X_val_eeg, X_val_bold, tv_y = data_utils.create_eeg_bold_pairs(eeg_val, bold_val)\n",
    "\n",
    "\n",
    "X_train_eeg = X_train_eeg.astype(np.float32)\n",
    "X_train_bold = X_train_bold.astype(np.float32)\n",
    "X_val_eeg = X_val_eeg.astype(np.float32)\n",
    "X_val_bold = X_val_bold.astype(np.float32)\n",
    "\n",
    "\n",
    "tr_y = np.array(tr_y, dtype=np.float32)\n",
    "tv_y = np.array(tv_y, dtype=np.float32)\n",
    "\n",
    "eeg_train = eeg_train.astype('float32')\n",
    "bold_train = bold_train.astype('float32')\n",
    "eeg_val = eeg_val.astype('float32')\n",
    "bold_val = bold_val.astype('float32')\n",
    "\n",
    "print(\"Pairs Created\")\n",
    "\n",
    "_, _, eeg_test, bold_test = data_utils.load_data(list(range(0)), list(range(12, 16)), \n",
    "                                                 bold_shift=bold_shift, \n",
    "                                                 n_partitions=n_partitions, \n",
    "                                                 f_resample=f_resample, \n",
    "                                                 roi=1, roi_ica_components=20)\n",
    "\n",
    "eeg_test, bold_test, _, _ = data_utils.standardize(eeg_test, bold_test, eeg_scaler=eeg_scaler, bold_scaler=bold_scaler)\n",
    "\n",
    "eeg_test = eeg_test.astype('float32')\n",
    "bold_test = bold_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_model = tf.keras.Sequential([tf.keras.layers.ConvLSTM2D(1, kernel_size=(5, 64), strides=(1, 1), activation='sigmoid', recurrent_activation='sigmoid', unit_forget_bias=True, return_sequences=True),\n",
    "                                tf.keras.layers.Reshape((14, 1))])\n",
    "\n",
    "bold_model = tf.keras.Sequential([tf.keras.layers.Conv2D(1, kernel_size=(2607, 1), strides=(1)),\n",
    "                                tf.keras.layers.Reshape((14, 1)),\n",
    "                                 tf.keras.layers.LSTM(1, return_sequences=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_eeg = X_train_eeg.reshape(X_train_eeg.shape[0], X_train_eeg.shape[3], X_train_eeg.shape[2], X_train_eeg.shape[1], X_train_eeg.shape[4])\n",
    "#X_val_eeg = X_val_eeg.reshape(X_val_eeg.shape[0], X_val_eeg.shape[3], X_val_eeg.shape[2], X_val_eeg.shape[1], X_val_eeg.shape[4])\n",
    "\n",
    "input_eeg = tf.keras.Input(X_train_eeg.shape[1:])\n",
    "processed_eeg = eeg_model(input_eeg)\n",
    "\n",
    "\n",
    "input_bold = tf.keras.Input(X_train_bold.shape[1:])\n",
    "processed_bold = bold_model(input_bold)\n",
    "\n",
    "correlation = tf.keras.layers.Lambda(losses.correlation_angle, \n",
    "                                     output_shape=losses.cos_dist_output_shape, \n",
    "                                     name=\"correlation_layer\")([processed_eeg, processed_bold])\n",
    "\n",
    "multi_modal_model = tf.keras.Model([input_eeg, input_bold], correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss=losses.contrastive_loss\n",
    "#optimizer = tf.keras.optimizers.Adam(lr=0.0001)#, clipvalue=0.1)\n",
    "optimizer = tf.keras.optimizers.RMSprop(lr=0.0001, rho=0.9, clipvalue=0.1)\n",
    "epochs=500\n",
    "batch_size=16\n",
    "\n",
    "class custom_training_loss:\n",
    "    def __init__(self):\n",
    "        self.encoder_loss = 0\n",
    "        self.decoder_loss = 0\n",
    "        self.batch = 0\n",
    "        \n",
    "    def update_batch_decoder_loss_avg(self, loss):\n",
    "        self.decoder_loss += loss\n",
    "        self.batch += 1\n",
    "    \n",
    "    def get_batch_decoder_loss_avg(self):\n",
    "        return self.decoder_loss/self.batch\n",
    "    \n",
    "    def update_batch_encoder_loss_avg(self, loss):\n",
    "        self.encoder_loss += loss\n",
    "        self.batch += 1\n",
    "    \n",
    "    def get_batch_encoder_loss_avg(self):\n",
    "        return self.encoder_loss/self.batch\n",
    "    \n",
    "def grad_multi_encoder(model, inputs, targets):\n",
    "    eeg = tf.convert_to_tensor(inputs[0])\n",
    "    bold = tf.convert_to_tensor(inputs[1])\n",
    "    with tf.GradientTape() as tape:\n",
    "        #tape.watch(model.variables)\n",
    "        tape.watch(eeg)\n",
    "        tape.watch(bold)\n",
    "        outputs = model([eeg, bold])\n",
    "        \n",
    "        encoder_loss = losses.contrastive_loss(outputs, targets)\n",
    "        \n",
    "        return encoder_loss,  tape.gradient(encoder_loss, \n",
    "                                            model.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Loss:  0.074091315 Validation Encoder Loss:  tf.Tensor(0.38848683, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.06875455 Validation Encoder Loss:  tf.Tensor(0.3390912, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.06738542 Validation Encoder Loss:  tf.Tensor(0.33787605, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.06698361 Validation Encoder Loss:  tf.Tensor(0.33778113, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.06683948 Validation Encoder Loss:  tf.Tensor(0.3462588, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.066780925 Validation Encoder Loss:  tf.Tensor(0.33776736, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.06674066 Validation Encoder Loss:  tf.Tensor(0.33973122, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.06672839 Validation Encoder Loss:  tf.Tensor(0.33598655, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.06671071 Validation Encoder Loss:  tf.Tensor(0.33482206, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.066699594 Validation Encoder Loss:  tf.Tensor(0.3347149, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.06669128 Validation Encoder Loss:  tf.Tensor(0.33468816, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.06668501 Validation Encoder Loss:  tf.Tensor(0.33524063, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.06668204 Validation Encoder Loss:  tf.Tensor(0.3347236, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.066680044 Validation Encoder Loss:  tf.Tensor(0.33511755, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.06667753 Validation Encoder Loss:  tf.Tensor(0.33492413, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.06667621 Validation Encoder Loss:  tf.Tensor(0.33466545, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.06667522 Validation Encoder Loss:  tf.Tensor(0.33529037, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.06667447 Validation Encoder Loss:  tf.Tensor(0.3597756, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.066673785 Validation Encoder Loss:  tf.Tensor(0.33583266, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.0666734 Validation Encoder Loss:  tf.Tensor(nan, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.066672884 Validation Encoder Loss:  tf.Tensor(0.33637965, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.06667255 Validation Encoder Loss:  tf.Tensor(0.3368818, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.066672176 Validation Encoder Loss:  tf.Tensor(nan, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.066672064 Validation Encoder Loss:  tf.Tensor(0.3352081, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.06667175 Validation Encoder Loss:  tf.Tensor(0.33614004, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.06667141 Validation Encoder Loss:  tf.Tensor(0.42675787, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.066671595 Validation Encoder Loss:  tf.Tensor(0.34110814, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.06667123 Validation Encoder Loss:  tf.Tensor(nan, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.0666709 Validation Encoder Loss:  tf.Tensor(nan, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.066671066 Validation Encoder Loss:  tf.Tensor(nan, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.0666709 Validation Encoder Loss:  tf.Tensor(0.34402227, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.06667061 Validation Encoder Loss:  tf.Tensor(nan, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.06667054 Validation Encoder Loss:  tf.Tensor(nan, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.066670455 Validation Encoder Loss:  tf.Tensor(nan, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.06667032 Validation Encoder Loss:  tf.Tensor(0.33595088, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.06667036 Validation Encoder Loss:  tf.Tensor(0.33496988, shape=(), dtype=float32)\n",
      "Encoder Loss:  0.06667019 Validation Encoder Loss:  tf.Tensor(0.33557692, shape=(), dtype=float32)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-292-70b8cd714d54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                                          [X_train_eeg[batch_start:batch_stop], \n\u001b[1;32m     15\u001b[0m                                                                   X_train_bold[batch_start:batch_stop]], \n\u001b[0;32m---> 16\u001b[0;31m                                                          tr_y[batch_start:batch_stop])\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#encoder_grads, _ = tf.clip_by_global_norm(encoder_grads, 0.1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-291-d52bfe3b7660>\u001b[0m in \u001b[0;36mgrad_multi_encoder\u001b[0;34m(model, inputs, targets)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meeg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meeg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mencoder_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrastive_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    677\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    678\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[0;32m--> 679\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    749\u001b[0m                                 ' implement a `call` method.')\n\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_internal_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m           \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    677\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    678\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[0;32m--> 679\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0;31m# `outputs` will be the inputs to the next layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow/python/keras/layers/convolutional_recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConvRNN2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    677\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    678\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[0;32m--> 679\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow/python/keras/layers/convolutional_recurrent.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[1;32m    936\u001b[0m                                         \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                                         \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m                                         initial_state=initial_state)\n\u001b[0m\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow/python/keras/layers/convolutional_recurrent.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state, constants)\u001b[0m\n\u001b[1;32m    391\u001b[0m                                          \u001b[0mgo_backwards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgo_backwards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                                          \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m                                          input_length=timesteps)\n\u001b[0m\u001b[1;32m    394\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m       \u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mrnn\u001b[0;34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask)\u001b[0m\n\u001b[1;32m   3850\u001b[0m           \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3851\u001b[0m           \u001b[0mloop_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_ta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3852\u001b[0;31m           **while_loop_kwargs)\n\u001b[0m\u001b[1;32m   3853\u001b[0m       \u001b[0mnew_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   3460\u001b[0m       \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# whether the body result was packed into a 1-item tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3462\u001b[0;31m       \u001b[0;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3463\u001b[0m         \u001b[0mloop_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3464\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basetuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i, lv)\u001b[0m\n\u001b[1;32m   3453\u001b[0m         \u001b[0mloop_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3454\u001b[0m         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\n\u001b[0;32m-> 3455\u001b[0;31m             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\n\u001b[0m\u001b[1;32m   3456\u001b[0m         \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fmri_eeg/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mlogical_and\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   5647\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[1;32m   5648\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5649\u001b[0;31m         \"LogicalAnd\", name, _ctx._post_execution_callbacks, x, y)\n\u001b[0m\u001b[1;32m   5650\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5651\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    _losses = custom_training_loss()\n",
    "\n",
    "    for batch_init in range(0, len(X_train_eeg), batch_size):\n",
    "        batch_start = batch_init\n",
    "        if(batch_start + batch_size >= len(X_train_eeg)):\n",
    "            batch_stop = len(X_train_eeg)\n",
    "        else:\n",
    "            batch_stop = batch_start + batch_size\n",
    "\n",
    "        #now train the compression by correlation model\n",
    "        encoder_loss, encoder_grads = grad_multi_encoder(multi_modal_model, \n",
    "                                                         [X_train_eeg[batch_start:batch_stop], \n",
    "                                                                  X_train_bold[batch_start:batch_stop]], \n",
    "                                                         tr_y[batch_start:batch_stop])\n",
    "        \n",
    "        #encoder_grads, _ = tf.clip_by_global_norm(encoder_grads, 0.1)\n",
    "        \n",
    "        with tf.name_scope(\"gradient_encoders\") as scope:\n",
    "            optimizer.apply_gradients(zip(encoder_grads, multi_modal_model.trainable_variables), name=scope)\n",
    "\n",
    "        # Track progress\n",
    "        _losses.update_batch_encoder_loss_avg(encoder_loss)\n",
    "        \n",
    "    # end epoch\n",
    "    encoder_loss = _losses.get_batch_encoder_loss_avg()\n",
    "\n",
    "    print(\"Encoder Loss: \", tf.keras.backend.eval(encoder_loss), \n",
    "          \"Validation Encoder Loss: \", losses.contrastive_loss(multi_modal_model([X_val_eeg, X_val_bold]),tv_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=122509649, shape=(), dtype=float32, numpy=0.3983344>"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses.contrastive_loss(multi_modal_model([X_val_eeg, X_val_bold]),tv_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
